{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1a3cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from sortedcontainers import SortedDict\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5065eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationStrategy:\n",
    "    def __init__(self, sorted_word_freq_pairs):\n",
    "        \"\"\"\n",
    "        Assume `sorted_word_freq_pairs` is in the form of a list of 2-element tuples, where each tuple is (word, frequency),\n",
    "        sorted in descending order by frequency (most frequent first).\n",
    "        \"\"\"\n",
    "        self.candidates = sorted_word_freq_pairs\n",
    "\n",
    "    def greedy_select(self):\n",
    "        return self.candidates[0][0]\n",
    "    \n",
    "    def temperature_based_select(self):\n",
    "        pass\n",
    "    \n",
    "    def randomly_select(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32682ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class N_Gram:\n",
    "    def __init__(self, corpus, tokenizer, n, start_sym = \"<s>\", end_sym = \"<s/>\"):\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n = n\n",
    "        self.lookup_table_by_word = {} # dict(context -> dict(potential_word -> count))\n",
    "        self.lookup_table_by_count = {} # dict(context -> list(tuple(potential_word, count)))\n",
    "        self.token_seq = []\n",
    "        self.start_sym = start_sym\n",
    "        self.end_sym = end_sym\n",
    "    \n",
    "    def tokenize(self, corpus):\n",
    "        \"\"\"\n",
    "        Tokenize the given corpus, including appropriate start and end symbols.\n",
    "        \"\"\"\n",
    "        return  [self.start_sym] * (self.n - 1) + self.tokenizer(corpus) + [self.end_sym] * (self.n - 1)\n",
    "    \n",
    "    def train(self, token_seq):\n",
    "        \"\"\"\n",
    "        Given a sequence of tokens (including adequate START and END tokens), populate the lookup_table\n",
    "        based on the n-grams (n-long word sequences) encountered in the corpus.\n",
    "        \"\"\"\n",
    "        # Populate the lookup_table for next word prediction by word\n",
    "        for i in range(self.n - 1, len(token_seq)):\n",
    "            cur, context = token_seq[i], token_seq[i-self.n+1:i-1]\n",
    "            if context not in self.lookup_table_by_word:\n",
    "                self.lookup_table_by_word[context] = defaultdict(int)\n",
    "            self.lookup_table_by_word[context][cur] += 1\n",
    "        \n",
    "        # Populate the lookup_table for next word prediction by count\n",
    "        for context in self.lookup_table_by_word:\n",
    "            self.lookup_table_by_count[context] = sorted(self.lookup_table_by_word[context].items(), key=lambda item: item[1], reverse=True)\n",
    "            \n",
    "    def sample(self, strategy):\n",
    "        \"\"\"\n",
    "        Repeatedly sample to generate a text autoregressively, starting from an empty input consisting only\n",
    "        of the start symbol.\n",
    "        \n",
    "        Follow the indicated strategy for how to sample.\n",
    "        \"\"\"\n",
    "        text_tokens = [self.start_sym] * (self.n - 1)\n",
    "        generated_token = None\n",
    "        \n",
    "        while generated_token != self.end_sym:\n",
    "            context = text_tokens[len(text_tokens) - self.n + 1:len(text_tokens)]\n",
    "            potential_words = self.lookup_table[context]\n",
    "            generated_token = strategy(potential_words)\n",
    "            text_tokens.append(generated_token)\n",
    "        \n",
    "        return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f10f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
